{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab6f83d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Graduation\n",
      "1: PhD\n",
      "2: Master\n",
      "3: 2n Cycle\n",
      "0: Single\n",
      "1: Married\n",
      "2: Divorced\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv(r'appian-x-iit-madras-hackathon-april-2025\\train.csv')\n",
    "test = pd.read_csv(r'appian-x-iit-madras-hackathon-april-2025\\test.csv')\n",
    "\n",
    "data.replace('Alone', 'Single', inplace=True)\n",
    "test.replace('Absurd', 'Single', inplace=True)\n",
    "data.replace('YOLO', 'Single', inplace=True)\n",
    "test.replace('YOLO', 'Single', inplace=True)\n",
    "test.replace('Alone', 'Single', inplace=True)\n",
    "data.replace('Together', 'Married', inplace=True)\n",
    "test.replace('Together', 'Married', inplace=True)\n",
    "\n",
    "test.replace('Basic', '2n Cycle', inplace=True)\n",
    "data.replace('Basic', '2n Cycle', inplace=True)\n",
    "test.replace('Widow', 'Divorced', inplace=True)\n",
    "data.replace('Widow', 'Divorced', inplace=True)\n",
    "\n",
    "data['Dt_Customer_1'] = pd.to_datetime(data['Dt_Customer'],format='mixed')\n",
    "data['Dt_Customer_1'] = data['Dt_Customer_1']-min(data['Dt_Customer_1'])\n",
    "data['Dates']=data['Dt_Customer_1'].dt.days\n",
    "\n",
    "test['Dt_Customer_1'] = pd.to_datetime(test['Dt_Customer'],format='mixed')\n",
    "test['Dt_Customer_1'] = test['Dt_Customer_1']-min(test['Dt_Customer_1'])\n",
    "test['Dates']=test['Dt_Customer_1'].dt.days\n",
    "Education = {}\n",
    "Marital_status = {}\n",
    "A = data['Education'].unique()\n",
    "B = data['Marital_Status'].unique()\n",
    "# A = test['Education'].unique()\n",
    "# B = test['Marital_Status'].unique()\n",
    "for i, category in enumerate(A):\n",
    "    l = [0]*len(A)\n",
    "    l[i] = 1\n",
    "    print(f\"{i}: {category}\")\n",
    "    Education[category] = i\n",
    "for i, category in enumerate(B):\n",
    "    l = [0]*len(B)\n",
    "    l[i] = 1\n",
    "    print(f\"{i}: {category}\")\n",
    "    Marital_status[category] = i\n",
    "data['Education'] = data['Education'].map(Education)\n",
    "test['Education'] = test['Education'].map(Education)\n",
    "data['Marital_Status'] = data['Marital_Status'].map(Marital_status)\n",
    "test['Marital_Status'] = test['Marital_Status'].map(Marital_status)\n",
    "\n",
    "# data = pd.get_dummies(data, columns=['Marital_Status', 'Education'], drop_first=True)\n",
    "# test = pd.get_dummies(test, columns=['Marital_Status', 'Education'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886dc5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ca3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "categorical_cols = ['Education', 'Marital_Status']\n",
    "numerical_cols = [\n",
    "    'Year_Birth', 'Income', 'Kidhome', 'Teenhome', 'Recency', 'Complain',\n",
    "    'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts',\n",
    "    'MntSweetProducts', 'MntGoldProds', 'NumDealsPurchases',\n",
    "    'AcceptedCmp1', 'AcceptedCmp2', 'AcceptedCmp3', 'AcceptedCmp4',\n",
    "    'AcceptedCmp5', 'NumWebPurchases', 'NumCatalogPurchases',\n",
    "    'NumStorePurchases', 'NumWebVisitsMonth', 'Dates'\n",
    "]\n",
    "lists = [\n",
    "    'Year_Birth',\n",
    "    'Income',\n",
    "    'Kidhome',\n",
    "    'Teenhome',\n",
    "    'Dates',\n",
    "    'Recency',\n",
    "    'MntWines',\n",
    "    'MntFruits',\n",
    "    'MntMeatProducts',\n",
    "    'MntFishProducts',\n",
    "    'MntSweetProducts',\n",
    "    'MntGoldProds',\n",
    "    'NumWebPurchases',\n",
    "    'NumCatalogPurchases',\n",
    "    'NumStorePurchases',\n",
    "    'NumDealsPurchases',\n",
    "    'NumWebVisitsMonth',\n",
    "    'AcceptedCmp1',\n",
    "    'AcceptedCmp2',\n",
    "    'AcceptedCmp3',\n",
    "    'AcceptedCmp4',\n",
    "    'AcceptedCmp5',\n",
    "    'Complain',\n",
    "    # 'Marital_Status_Married',\n",
    "    # 'Marital_Status_Single',\n",
    "    # 'Education_Graduation',\n",
    "    # 'Education_Master',\n",
    "    # 'Education_PhD',\n",
    "    'Marital_Status',\n",
    "    'Education'\n",
    "    # 'Target'\n",
    "]\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomerDataset(Dataset):\n",
    "    def __init__(self, df, feature_cols, target_col='Target'):\n",
    "        self.X = df[feature_cols].values.astype('float32')\n",
    "        self.y = df[target_col].values.astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': torch.tensor(self.X[idx], dtype=torch.float),\n",
    "            'target': torch.tensor(self.y[idx], dtype=torch.float)\n",
    "        }\n",
    "        # return torch.tensor(self.X[idx], dtype=torch.float), torch.tensor(self.y[idx], dtype=torch.float)\n",
    "\n",
    "means = data.mean(numeric_only=True)\n",
    "default_values = means[lists[:-5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e48354e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomerDataset(data, lists, target_col='Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8387d09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ImputationLayer(nn.Module):\n",
    "    def __init__(self, default_values):\n",
    "        super(ImputationLayer, self).__init__()\n",
    "        self.impute = nn.Parameter(torch.tensor(default_values, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = torch.isnan(x)\n",
    "        x[mask] = self.impute.expand(x.shape[0], -1)[mask]\n",
    "        return x\n",
    "\n",
    "class TrainableScaler(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(TrainableScaler, self).__init__()\n",
    "        self.mean = nn.Parameter(torch.zeros(num_features))\n",
    "        self.std = nn.Parameter(torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "class MarketResearchModel(nn.Module):\n",
    "    def __init__(self, num_numeric_features, emb_sizes = [3,4], layers_list=[64, 32, 1], default_values=default_values):\n",
    "        super(MarketResearchModel, self).__init__()\n",
    "\n",
    "        # Imputation and scaling layers\n",
    "        self.imputer = ImputationLayer(default_values)  # Replace with actual means\n",
    "        self.scaler = TrainableScaler(num_features=num_numeric_features)\n",
    "\n",
    "        # Embedding layers\n",
    "        self.embedding_1 = nn.Embedding(num_embeddings=emb_sizes[0], embedding_dim=5)  # for class feature -6 to -3\n",
    "        self.embedding_2 = nn.Embedding(num_embeddings=emb_sizes[1], embedding_dim=5)  # for class feature -3 to -1\n",
    "\n",
    "        # Input size for FFN\n",
    "        input_size = num_numeric_features + 10\n",
    "\n",
    "        # Build feedforward layers dynamically\n",
    "        layers = []\n",
    "        in_dim = input_size\n",
    "        for out_dim in layers_list:\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if out_dim != 1:\n",
    "                layers.append(nn.BatchNorm1d(out_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                layers.append(nn.Dropout(0.3))\n",
    "            in_dim = out_dim\n",
    "        if layers_list[-1] != 1:\n",
    "            layers.append(nn.Linear(in_dim, 1))\n",
    "        layers.append(nn.Sigmoid())\n",
    "\n",
    "        self.ff = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_numeric = x[:, :-2]\n",
    "        # x_cat1 = x[:, -5:-3].long()\n",
    "        # x_cat2 = x[:, -3:-1].long()\n",
    "        x_cat1 = x[:,-2:-1].long()\n",
    "        x_cat2 = x[:,-1:].long()\n",
    "        # print(x_numeric.shape, x_cat1.shape, x_cat2.shape)\n",
    "        x_numeric = self.imputer(x_numeric)\n",
    "        x_numeric = self.scaler(x_numeric)\n",
    "\n",
    "        emb1 = self.embedding_1(x_cat1).squeeze(1)\n",
    "        emb2 = self.embedding_2(x_cat2).squeeze(1)\n",
    "        # print(emb1.shape, emb2.shape, x_numeric.shape)\n",
    "        x = torch.cat([x_numeric, emb1, emb2], dim=1)\n",
    "        return self.ff(x).squeeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0381df59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(default_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e77973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monar\\AppData\\Local\\Temp\\ipykernel_17244\\2147856230.py:8: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  self.impute = nn.Parameter(torch.tensor(default_values, dtype=torch.float32), requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "means = data.mean(numeric_only=True)\n",
    "default_values = means[lists[:-2]]\n",
    "model = MarketResearchModel(num_numeric_features=len(lists)-2, emb_sizes=[3,4], layers_list=[64, 32, 1], default_values=default_values).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "\n",
    "summary(model, input_size=(3, len(lists)), col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\", \"mult_adds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c06a631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Training & Evaluation ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_all, y = batch[\"features\"], batch[\"target\"]\n",
    "        x = x_all[:, :-2]\n",
    "        cat1 = x_all[:, -2].long()\n",
    "        cat2 = x_all[:, -1].long()\n",
    "        x, y, cat1, cat2 = x.to(device), y.to(device), cat1.to(device), cat2.to(device)\n",
    "        x_all = x_all.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_all)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(y)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_all, y = batch[\"features\"], batch[\"target\"]\n",
    "            x = x_all[:, :-2]\n",
    "            cat1 = x_all[:, -2].long()\n",
    "            cat2 = x_all[:, -1].long()\n",
    "            x, y, cat1, cat2 = x.to(device), y.to(device), cat1.to(device), cat2.to(device)\n",
    "            x_all = x_all.to(device)\n",
    "            pred = model(x_all)\n",
    "            all_preds.append(pred.cpu())\n",
    "            all_labels.append(y.cpu())\n",
    "\n",
    "    preds = torch.cat(all_preds) > 0.5\n",
    "    labels = torch.cat(all_labels)\n",
    "    return accuracy_score(labels.numpy(), preds.numpy())\n",
    "\n",
    "# --- Hyperparameter Tuning ---\n",
    "def run_tuning(dataset, emb_sizes, param_grid, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    best_acc = 0\n",
    "    best_model_state = None\n",
    "    best_config = None\n",
    "\n",
    "    val_len = int(0.2 * len(dataset))\n",
    "    train_len = len(dataset) - val_len\n",
    "    train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
    "\n",
    "    for config in ParameterGrid(param_grid):\n",
    "        print(f\"\\nTraining config: {config}\")\n",
    "        train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=config['batch_size'])\n",
    "        model = MarketResearchModel(\n",
    "            num_numeric_features=dataset[0][\"features\"].shape[0] - 2,\n",
    "            emb_sizes=emb_sizes,\n",
    "            layers_list=config['layers_list']\n",
    "        ).to(device)\n",
    "        # print(dataset[0][\"features\"].shape[0] - 2, emb_sizes, config['layers_list'])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        for epoch in range(config['epochs']):\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_acc = evaluate(model, val_loader, device)\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f} | Val Acc = {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_state = model.state_dict()\n",
    "            best_config = config\n",
    "\n",
    "    print(f\"\\nBest Config: {best_config} | Best Validation Accuracy: {best_acc:.4f}\")\n",
    "    return best_model_state, best_config\n",
    "\n",
    "# --- Example Run ---\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4, 1e-4],                          # Learning rates\n",
    "    'batch_size': [32, 64, 128],                       # Batch sizes\n",
    "    'layers_list': [                                   # Network depths\n",
    "        [64, 32],\n",
    "        [128, 64, 32],\n",
    "        [256, 128, 64, 32],\n",
    "        [128, 128, 64, 32]\n",
    "    ],\n",
    "    'epochs': [15, 25],                                # Training duration\n",
    "}\n",
    "\n",
    "# To run: best_state, best_params = run_tuning(dataset, emb_sizes=[3, 4], param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7d25ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Training & Evaluation ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_all, y = batch[\"features\"], batch[\"target\"]\n",
    "        x = x_all[:, :-2]\n",
    "        cat1 = x_all[:, -2].long()\n",
    "        cat2 = x_all[:, -1].long()\n",
    "        x, y, cat1, cat2 = x.to(device), y.to(device), cat1.to(device), cat2.to(device)\n",
    "        x_all = x_all.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x_all)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(y)\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "# Reuse evaluate for both train and validation\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x_all, y = batch[\"features\"], batch[\"target\"]\n",
    "            x = x_all[:, :-2]\n",
    "            cat1 = x_all[:, -2].long()\n",
    "            cat2 = x_all[:, -1].long()\n",
    "            x, y, cat1, cat2 = x.to(device), y.to(device), cat1.to(device), cat2.to(device)\n",
    "            x_all = x_all.to(device)\n",
    "            pred = model(x_all)\n",
    "            all_preds.append(pred.cpu())\n",
    "            all_labels.append(y.cpu())\n",
    "    preds = torch.cat(all_preds) > 0.5\n",
    "    labels = torch.cat(all_labels)\n",
    "    return accuracy_score(labels.numpy(), preds.numpy())\n",
    "\n",
    "# --- Hyperparameter Tuning ---\n",
    "def run_tuning(dataset, emb_sizes, param_grid, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    best_global_score = 0\n",
    "    best_model_state = None\n",
    "    best_config = None\n",
    "\n",
    "    val_len = int(0.2 * len(dataset))\n",
    "    train_len = len(dataset) - val_len\n",
    "    train_ds, val_ds = random_split(dataset, [train_len, val_len])\n",
    "\n",
    "    for config in ParameterGrid(param_grid):\n",
    "        print(f\"\\nTraining config: {config}\")\n",
    "        train_loader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)\n",
    "        val_loader = DataLoader(val_ds, batch_size=config['batch_size'])\n",
    "\n",
    "        model = MarketResearchModel(\n",
    "            num_numeric_features=dataset[0][\"features\"].shape[0] - 2,\n",
    "            emb_sizes=emb_sizes,\n",
    "            layers_list=config['layers_list']\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config['lr'])\n",
    "        criterion = nn.BCELoss()\n",
    "\n",
    "        best_config_score = 0\n",
    "        for epoch in range(config['epochs']):\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            train_acc = evaluate(model, train_loader, device)\n",
    "            val_acc = evaluate(model, val_loader, device)\n",
    "            epoch_score = min(train_acc, val_acc)\n",
    "            best_config_score = max(best_config_score, epoch_score)\n",
    "            # print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f} | Train Acc={train_acc:.4f} | Val Acc={val_acc:.4f} | Min Acc={epoch_score:.4f}\")\n",
    "\n",
    "        print(f\"Best Min(train, val) accuracy for config: {best_config_score:.4f}\")\n",
    "\n",
    "        if best_config_score > best_global_score:\n",
    "            best_global_score = best_config_score\n",
    "            best_model_state = model.state_dict()\n",
    "            best_config = config\n",
    "\n",
    "    print(f\"\\nBest Config: {best_config} | Best Min Acc: {best_global_score:.4f}\")\n",
    "    return best_model_state, best_config\n",
    "\n",
    "# --- Example Run ---\n",
    "param_grid = {\n",
    "    'lr': [1e-3, 5e-4, 1e-4],                          # Learning rates\n",
    "    'batch_size': [32, 64, 128],                       # Batch sizes\n",
    "    'layers_list': [                                   # Network depths\n",
    "        [64, 32],\n",
    "        [128, 64, 32],\n",
    "        [256, 128, 64, 32],\n",
    "        [128, 128, 64, 32]\n",
    "    ],\n",
    "    'epochs': [25, 35],                                # Training duration\n",
    "}\n",
    "\n",
    "# To run: best_state, best_params = run_tuning(dataset, emb_sizes=[3, 4], param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8256466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training config: {'batch_size': 32, 'epochs': 25, 'layers_list': [64, 32], 'lr': 0.001}\n",
      "Best Min(train, val) accuracy for config: 0.7987\n",
      "\n",
      "Training config: {'batch_size': 32, 'epochs': 25, 'layers_list': [64, 32], 'lr': 0.0005}\n",
      "Best Min(train, val) accuracy for config: 0.7879\n",
      "\n",
      "Training config: {'batch_size': 32, 'epochs': 25, 'layers_list': [64, 32], 'lr': 0.0001}\n",
      "Best Min(train, val) accuracy for config: 0.7321\n",
      "\n",
      "Training config: {'batch_size': 32, 'epochs': 25, 'layers_list': [128, 64, 32], 'lr': 0.001}\n",
      "Best Min(train, val) accuracy for config: 0.8051\n",
      "\n",
      "Training config: {'batch_size': 32, 'epochs': 25, 'layers_list': [128, 64, 32], 'lr': 0.0005}\n",
      "Best Min(train, val) accuracy for config: 0.7796\n",
      "\n",
      "Training config: {'batch_size': 32, 'epochs': 25, 'layers_list': [128, 64, 32], 'lr': 0.0001}\n",
      "Best Min(train, val) accuracy for config: 0.7380\n",
      "\n",
      "Training config: {'batch_size': 32, 'epochs': 25, 'layers_list': [256, 128, 64, 32], 'lr': 0.001}\n",
      "Best Min(train, val) accuracy for config: 0.8179\n",
      "\n",
      "Training config: {'batch_size': 32, 'epochs': 25, 'layers_list': [256, 128, 64, 32], 'lr': 0.0005}\n",
      "Best Min(train, val) accuracy for config: 0.7796\n",
      "\n",
      "Training config: {'batch_size': 32, 'epochs': 25, 'layers_list': [256, 128, 64, 32], 'lr': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "run_tuning(dataset, emb_sizes=[3, 4], param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2487f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
